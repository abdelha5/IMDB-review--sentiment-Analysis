{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {},
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0055a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b7e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.read_csv('movie_review_train.csv')\n",
    "test_file = pd.read_csv('movie_review_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8a6ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728acf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=ToktokTokenizer()\n",
    "\n",
    "def normalize_data(train_file, test_file):    \n",
    "    tokenizer= ToktokTokenizer()\n",
    "    #Setting English stopwords\n",
    "    stopword_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    def strip_html(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "    #Removing the square brackets\n",
    "    def remove_between_square_brackets(text):\n",
    "        return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "    #Removing the noisy text\n",
    "    def denoise_text(text):\n",
    "        text = strip_html(text)\n",
    "        text = remove_between_square_brackets(text)\n",
    "        return text\n",
    "\n",
    "    train_file['review']= train_file['review'].apply(denoise_text)\n",
    "    test_file['review']= test_file['review'].apply(denoise_text)\n",
    "\n",
    "\n",
    "    #Define function for removing special characters\n",
    "    def remove_special_characters(text, remove_digits=True):\n",
    "        pattern=r'[^a-zA-z0-9\\s]'\n",
    "        text=re.sub(pattern,'',text)\n",
    "        return text\n",
    "    #Apply function on review column\n",
    "    train_file['review'] = train_file['review'].apply(remove_special_characters)\n",
    "    test_file['review'] = test_file['review'].apply(remove_special_characters)\n",
    "\n",
    "    # remove whitespace\n",
    "    train_file['review'] = train_file['review'].apply(lambda x: x.strip())\n",
    "    test_file['review'] = test_file['review'].apply(lambda x: x.strip())\n",
    "\n",
    "        \n",
    "    # convert to lower\n",
    "    train_file['review'] = train_file['review'].apply(lambda x: x.lower())\n",
    "    test_file['review'] = test_file['review'].apply(lambda x: x.lower())\n",
    "\n",
    "    #Stemming the text\n",
    "    def simple_stemmer(text):\n",
    "        ps= nltk.porter.PorterStemmer()\n",
    "        text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    #Apply function on review column\n",
    "    train_file['review'] = train_file['review'].apply(simple_stemmer)\n",
    "    test_file['review'] = test_file['review'].apply(simple_stemmer)\n",
    "\n",
    "    stop=set(stopwords.words('english'))\n",
    "    print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "    def remove_stopwords(text, is_lower_case=False):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        if is_lower_case:\n",
    "            filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "        else:\n",
    "            filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "        filtered_text = ' '.join(filtered_tokens)    \n",
    "        return filtered_text\n",
    "\n",
    "#Apply function on review column\n",
    "    train_file['review']=train_file['review'].apply(remove_stopwords)\n",
    "    test_file['review']=test_file['review'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "    X_train = train_file['review']\n",
    "    X_test = test_file['review']\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0ac481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    # Read the CSV file and extract Bag of Words Features\n",
    "    X_train, X_test = normalize_data(train_file,test_file)\n",
    "\n",
    "    #Count vectorizer for bag of words\n",
    "    cv = CountVectorizer(analyzer = \"word\", tokenizer = None, max_features = 5000) \n",
    "    #cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "    #transformed train reviews\n",
    "    X_train = cv.fit_transform(X_train)\n",
    "    #transformed test reviews\n",
    "    X_test = cv.transform(X_test)\n",
    "\n",
    "    #labeling the sentient data\n",
    "    lb = LabelBinarizer()\n",
    "    #transformed sentiment data\n",
    "    y_train = lb.fit_transform(train_file['sentiment'])\n",
    "    y_test = lb.fit_transform(test_file['sentiment'])\n",
    "    #y_train = train_file['review']\n",
    "    #y_test = test_file['sentiment']\n",
    "    \n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d98fab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ma', 'wouldn', 'their', 'off', 'its', 'not', 'or', 'between', 'mustn', 'have', 't', \"didn't\", 'myself', 'are', 'll', \"you'd\", 'about', 'yourself', 'now', \"weren't\", 'does', 'her', 'down', 'he', \"that'll\", 'me', 'to', 'more', 'being', 'how', 'shan', 'our', 'through', 'when', \"mightn't\", 'shouldn', \"wouldn't\", 'again', 'those', 'any', 'no', 'should', 've', \"it's\", 'just', 'ourselves', 'only', 'can', 'further', 'd', 'doesn', 'all', 'in', 'by', 're', 'than', \"couldn't\", 'few', \"she's\", 'doing', 'who', \"hasn't\", 'isn', 'where', 'each', \"don't\", 'whom', \"aren't\", 'itself', 'needn', 'ours', 'the', 'why', 'won', \"haven't\", 'such', 'before', 's', 'too', 'had', 'there', 'own', 'above', 'mightn', 'out', 'himself', 'up', 'some', 'were', 'until', 'that', 'under', \"shouldn't\", 'with', 'be', 'most', 'very', 'these', \"you'll\", 'a', \"wasn't\", 'yourselves', 'after', 'both', 'other', 'will', \"you're\", 'my', 'ain', 'having', 'hasn', 'during', 'at', 'your', 'which', 'of', 'and', \"hadn't\", 'we', 'them', 'what', 'haven', 'him', 'once', 'hers', \"isn't\", 'it', \"needn't\", \"won't\", 'she', 'then', \"you've\", 'been', 'couldn', 'they', 'don', 'y', 'i', 'theirs', 'against', 'didn', 'so', 'do', 'an', 'themselves', 'from', 'because', 'did', 'while', 'wasn', 'same', 'aren', \"should've\", 'was', 'nor', 'weren', \"mustn't\", 'into', 'over', 'below', 'has', 'o', 'this', 'on', 'm', \"shan't\", 'here', 'his', 'herself', 'am', 'but', 'you', 'if', \"doesn't\", 'as', 'for', 'hadn', 'yours', 'is'}\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = extract_bag_of_words_train_test(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11b06bd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pz/30hf_0g10s1550w4s29shz4r0000gn/T/ipykernel_8348/3528615476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdecision_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m456\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Bias calculation with 8-1 loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "decision_tree = tree.DecisionTreeClassifier(max_depth=2, random_state=456).fit(X_train, y_train)\n",
    "# Bias calculation with 8-1 loss\n",
    "predictions = decision_tree.predict(X_train)\n",
    "print(\"Algorithm: Decision Tree\")\n",
    "print(\"0-1 Loss\" + str(np.round(zero_one_loss(predictions, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e311b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493333333333334"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try with the GRADIENT BOOSTING CLASSIFIER\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf=GradientBoostingClassifier(learning_rate = 0.5, max_depth = 2, max_features = 5000, n_estimators=300,random_state= 0)\n",
    "clf.fit(X_train,y_train.ravel())\n",
    "clf.score(X_test ,y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "788e63e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingClassifier(learning_rate=0.5,\n",
       "                                                  max_depth=2,\n",
       "                                                  max_features=5000,\n",
       "                                                  n_estimators=300,\n",
       "                                                  random_state=0),\n",
       "             param_grid={'n_estimators': [80, 100]})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "mod=GridSearchCV(clf,param_grid={'n_estimators': [80,100]})\n",
    "mod.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "737c449e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493333333333334"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientBoostingClassifier(learning_rate=0.25, max_depth=2, max_features=5000,\n",
    "                           n_estimators=80, random_state=0)\n",
    "clf.fit(X_train,y_train.ravel())\n",
    "clf.score(X_test,y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0aa05eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8513333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       731\n",
      "           1       0.85      0.86      0.86       769\n",
      "\n",
      "    accuracy                           0.85      1500\n",
      "   macro avg       0.85      0.85      0.85      1500\n",
      "weighted avg       0.85      0.85      0.85      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[618 113]\n",
      " [110 659]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions using adaboost for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# define dataset\n",
    "\n",
    "# define the model\n",
    "model = AdaBoostClassifier(n_estimators= 400, learning_rate = 0.5, random_state=0)\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X_train, y_train.ravel())\n",
    "# make a single prediction\n",
    "#predict = model.predict(y_test)\n",
    "print(model.score(X_test,y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bbabf1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807 (0.016)\n",
      "Accuracy: 84.07%\n"
     ]
    }
   ],
   "source": [
    "# train-test split evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier(use_label_encoder= False, max_depth = 1, verbosity = 0, tree_method = 'hist')\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X_train, y_train.ravel(), scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "model = XGBClassifier(n_estimators = 260, use_label_encoder= False,  max_depth = 10, verbosity = 0, tree_method = 'hist')\n",
    "model.fit(X_train, y_train.ravel())\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "776b099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802 (0.017)\n",
      "Accuracy: 81.33%\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting for classification in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = GradientBoostingClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X_train, y_train.ravel(), scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train.ravel())\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "# make a single prediction\n",
    "#yhat = model.predict(row)\n",
    "#print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94c07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        import numpy as np\n",
    "        from sklearn import svm\n",
    "        #implement initialisation\n",
    "        self.some_paramter=1\n",
    "    # define your own kernel here\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html\n",
    "    def fit(self, X,y):\n",
    "        # training of the SVM\n",
    "        # Ensure you call your own defined kernel here\n",
    "        return\n",
    "    def predict(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        return    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89603f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3805e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    def __init__(self):\n",
    "        import numpy as np\n",
    "        #implement initialisation\n",
    "        self.some_paramter=1\n",
    "    def fit(self, X,y):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        import numpy as np\n",
    "        #implement training of the boosting classifier\n",
    "        return \n",
    "    def predict(self, X):\n",
    "        # implement prediction of the boosting classifier\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4632591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    Y_Pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c27de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
