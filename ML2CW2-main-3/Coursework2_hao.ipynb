{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {},
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ac481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    import numpy as np\n",
    "    from bs4 import BeautifulSoup\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    def tokenize(text): \n",
    "        tknzr = TweetTokenizer()\n",
    "        return tknzr.tokenize(text)\n",
    "\n",
    "    def stem(doc):\n",
    "        from nltk.stem import PorterStemmer\n",
    "        stemmer = PorterStemmer()\n",
    "        return (stemmer.stem(w) for w in analyzer(doc))\n",
    "    # Read the CSV file and extract Bag of Words Features\n",
    "    train_file = pd.read_csv('movie_review_train.csv')\n",
    "    test_file = pd.read_csv('movie_review_test.csv')\n",
    "\n",
    "    train_file['review'] = train_file['review'].apply(lambda x: BeautifulSoup(x, \"lxml\").text)\n",
    "    test_file['review'] = test_file['review'].apply(lambda x: BeautifulSoup(x, \"lxml\").text)\n",
    "\n",
    "    train_file['sentiment'] = train_file['sentiment'].apply(lambda x: 1 if x=='negative' else 0)\n",
    "    test_file['sentiment'] = test_file['sentiment'].apply(lambda x: 1 if x=='negative' else 0)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    train_file['review'] = train_file['review'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    test_file['review'] = test_file['review'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "\n",
    "\n",
    "    \n",
    "    en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "    \n",
    "    \n",
    "    analyzer = CountVectorizer().build_analyzer()\n",
    "    vectorizer = CountVectorizer(\n",
    "    analyzer = stem,\n",
    "    tokenizer = tokenize,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = en_stopwords)\n",
    "\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_file['review'].values)\n",
    "    X_test = vectorizer.transform(test_file['review'].values)   \n",
    "    return X_train, train_file['sentiment'], X_test, test_file['sentiment']\n",
    "    \n",
    "train_file = open(\"movie_review_train.csv\", \"r\")\n",
    "test_file = open(\"movie_review_test.csv\", \"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haochen\\miniconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernal  linear\n",
      "Learning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "def linear_kernel(x_i, x_j):\n",
    "    print(x_i.dot(x_j.T))\n",
    "    return x_i.dot(x_j.T)\n",
    "\n",
    "def my_kernel(X, Y):\n",
    "    return np.dot(X, Y.T)\n",
    "\n",
    "def hyperbolic_tangent_kernel(x_i, x_j):\n",
    "    return np.tanh(x_i.dot(x_j.T))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_gram(x1, x2, kernel):\n",
    "    return np.array([[kernel(_x1, _x2) for _x2 in x2] for _x1 in x1])\n",
    "\n",
    "    \n",
    "def RBF(x1, x2, gamma  = 1):\n",
    "    return np.exp(-gamma*np.linalg.norm(x1-x2))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "get_gram(X_train, X_train, RBF)\n",
    "clf_svm =  SVC(C=0.01, probability=True, kernel='precomputed', class_weight=\"balanced\")\n",
    "\n",
    "print(\"Learning\")\n",
    "clf_svm.fit(X_train, y_train)\n",
    "print(\"Predicting\")\n",
    "clf_svm.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Learning\")\n",
    "clf_svm.fit(get_gram(X_train, X_train, RBF), y_train)\n",
    "print(\"Predicting\")\n",
    "clf_svm.score(X_test, y_test)\n",
    "\n",
    "\n",
    "for kernel in (\"linear\", \"poly\", \"rbf\"):\n",
    "    clf = SVC(C=0.01, kernel=kernel, class_weight=\"balanced\", probability= True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Kernel: \", kernel)\n",
    "    print(clf.score(X_test, y_test))\n",
    "\"\"\"\n",
    "X_train, y_train, X_test, y_test = extract_bag_of_words_train_test(train_file, test_file)\n",
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "\n",
    "for kernel in (\"linear\", \"poly\", \"rbf\"):\n",
    "    clf = SVC(kernel=kernel, class_weight=\"balanced\", probability= True)\n",
    "    grid_svm = GridSearchCV(estimator=clf,\n",
    "                    param_grid = {'C': [0.01, 0.05, 0.1, 1, 10], 'gamma': [1,0.1,0.01,0.001]}, \n",
    "                    cv = kfolds,\n",
    "                    scoring=\"roc_auc\",\n",
    "                    verbose=1,   \n",
    "                    n_jobs=-1) \n",
    "    print(\"Kernal \", kernel)\n",
    "    print(\"Learning\")\n",
    "    grid_svm.fit(X_train, y_train)\n",
    "    print(\"Predicting\")\n",
    "    print(grid_svm.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, class_weight='balanced', gamma=0.001, probability=True)\n"
     ]
    }
   ],
   "source": [
    "print(grid_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94c07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        import numpy as np\n",
    "        from sklearn import svm\n",
    "        from sklearn.svm import SVC\n",
    "        #implement initialisation\n",
    "    # define your own kernel here\n",
    "        self.clf = SVC(C=0.01, class_weight='balanced', probability=True, kernel = 'linear')\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html\n",
    "    def fit(self, X, y):\n",
    "        # training of the SVM\n",
    "        # Ensure you call your own defined kernel here\n",
    "        self.clf.fit(X, y)\n",
    "        return None\n",
    "\n",
    "    def predict(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        y_pred = self.clf.predict(X)\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89603f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ffd4adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haochen\\miniconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8613333333333333\n"
     ]
    }
   ],
   "source": [
    "acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f611e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haochen\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\bs4\\__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = extract_bag_of_words_train_test(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8513333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import base\n",
    "\n",
    "\n",
    "\n",
    "class AdaBoostClassifier:\n",
    "    \n",
    "    def __init__(self,n_estimators=50, max_depth = 1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.trees = np.zeros(shape=self.n_estimators, dtype=object)\n",
    "        self.max_depth = max_depth\n",
    "        self.base_estimator = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "        self.estimator_errors_ = np.zeros(shape = n_estimators)\n",
    "        self.treees = []\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        n = len(y)\n",
    "        weights = np.ones(shape=n) / n\n",
    "\n",
    "        for idx in range(self.n_estimators):\n",
    "            learner_tree = base.clone(self.base_estimator)\n",
    "            weak_learner_pred = learner_tree.fit(X,y,sample_weight=weights).predict\n",
    "            err = np.average((weak_learner_pred(X) != y),weights=weights,axis=0)\n",
    "            self.estimator_errors_[idx] = err\n",
    "            learner_weight = (np.log((1-err)/err)+np.log(2-1))\n",
    "            weights *= np.exp(learner_weight*(weak_learner_pred(X) != y)*(weights > 0))\n",
    "            self.trees[idx] = (learner_weight, weak_learner_pred)\n",
    "\n",
    "        \n",
    "    def vector(self, y):\n",
    "        y_vec = []\n",
    "        for yi in y:\n",
    "            v = np.ones(2)*(-1/(2-1))\n",
    "            v[yi] = 1\n",
    "            y_vec.append(v)\n",
    "        return np.array(y_vec)\n",
    "\n",
    "\n",
    "    def predict(self,X):\n",
    "        k = 2\n",
    "        y_pred = sum(learner_weight * self.vector(weak_learner_pred(X)) \n",
    "        for learner_weight, weak_learner_pred in self.trees)\n",
    "        \n",
    "        return np.argmax(y_pred,axis=1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "dt = AdaBoostClassifier(n_estimators=500, max_depth = 4)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a343f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Best N', 10, 'Best depth', 1] 0.7073333333333334\n",
      "['Best N', 10, 'Best depth', 2] 0.7513333333333333\n",
      "['Best N', 10, 'Best depth', 3] 0.768\n",
      "['Best N', 10, 'Best depth', 4] 0.7813333333333333\n",
      "['Best N', 20, 'Best depth', 3] 0.7933333333333333\n",
      "['Best N', 20, 'Best depth', 4] 0.802\n",
      "['Best N', 20, 'Best depth', 5] 0.8066666666666666\n",
      "['Best N', 30, 'Best depth', 3] 0.8126666666666666\n",
      "['Best N', 30, 'Best depth', 5] 0.8186666666666667\n",
      "['Best N', 40, 'Best depth', 3] 0.822\n",
      "['Best N', 40, 'Best depth', 5] 0.8306666666666667\n",
      "['Best N', 100, 'Best depth', 5] 0.8326666666666667\n",
      "['Best N', 500, 'Best depth', 1] 0.8366666666666667\n",
      "['Best N', 500, 'Best depth', 2] 0.8393333333333334\n",
      "['Best N', 500, 'Best depth', 3] 0.8453333333333334\n",
      "['Best N', 500, 'Best depth', 4] 0.8533333333333334\n",
      "['Best N', 500, 'Best depth', 4] 0.8533333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters={'iteration': [10,20,30,40,50,100,500], 'max_depth':[1,2,3,4,5,10,15,20]}\n",
    "acc_best = 0\n",
    "for key in parameters:\n",
    "    for value in parameters[key]:\n",
    "        if key == 'iteration':\n",
    "            for i in range(len(parameters['max_depth'])):\n",
    "                dt = AdaBoostClassifier(n_estimators=value, max_depth = parameters['max_depth'][i])\n",
    "                dt.fit(X_train, y_train)\n",
    "                y_pred = dt.predict(X_test)\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                if acc >acc_best:\n",
    "                    acc_best = acc\n",
    "                    best_parameter = ['Best N', value, 'Best depth', parameters['max_depth'][i]]\n",
    "                    print(best_parameter, acc)\n",
    "        else:\n",
    "            for i in range(len(parameters['iteration'])):\n",
    "                dt = AdaBoostClassifier(n_estimators=parameters['iteration'][i], max_depth = value)\n",
    "                dt.fit(X_train, y_train)\n",
    "                y_pred = dt.predict(X_test)\n",
    "                acc = accuracy_score(y_test, y_pred)     \n",
    "                if acc >acc_best:\n",
    "                    acc_best = acc\n",
    "                    best_parameter = ['Best N', parameters['iteration'][i], 'Best depth', value]\n",
    "                    print(best_parameter, acc)\n",
    "\n",
    "print(best_parameter, acc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2688 candidates, totalling 13440 fits\n",
      "0.7866639745685675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,2,5,7,9,10,12],\n",
    "           \"min_samples_leaf\":[2,3,4,5],\n",
    "           \"max_leaf_nodes\":[None,10,20,30,40,50], 'criterion':['entropy', 'gini'],'max_features':[None, 'auto', 'sqrt','log2']}\n",
    "reg_decision_model = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "tuning_model = GridSearchCV(reg_decision_model,param_grid=parameters,scoring=\"roc_auc\",cv=kfolds,verbose=1, n_jobs= -1)\n",
    "tuning_model.fit(X_train,y_train)\n",
    "print(tuning_model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=12, max_features='sqrt', min_samples_leaf=5)\n"
     ]
    }
   ],
   "source": [
    "print(tuning_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4632591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    Y_Pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d6c27de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
